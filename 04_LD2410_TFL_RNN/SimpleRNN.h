#include <Arduino.h>
#include <math.h>

// Размерности:
// Длина временной последовательности.
#define TIMESTEPS 10
// Число признаков на каждом временном шаге (9 динамических + 9 статических сигналов).
#define FEATURES 18
// Количество скрытых нейронов в RNN.
#define HIDDEN 64
// Количество выходных классов, которые модель может предсказывать.
#define OUTPUT 5

// Кол-во классов предсказываемых моделью.
constexpr int kCategoryCount = 5;

// Наименования категорий, которые модель может классифицировать.
const char* kCategoryLabels[kCategoryCount] = {"empty", "jump", "run", "squat", "stand"};

/*
Вектор скрытого состояния RNN (h_t).
Хранит текущую "память" сети и обновляется на каждом временном шаге.*/
float hidden[HIDDEN];

/*
Реализация реккурентной нейронной сети:
  input — матрица входных данных размера [TIMESTEPS][FEATURES].
  output — массив для хранения результата [OUTPUT] (вероятности классов).*/
void simpleRNN(float input[TIMESTEPS][FEATURES], float output[OUTPUT]) {
  // Так как вначале последовательности вектор скрытого состояния "память сети" пуст, то следует все его значения приравнять к нулю "сбросить".
  for (int i = 0; i < HIDDEN; i++) hidden[i] = 0;

  // Пройдём по данным вдоль временной оси.
  for (int t = 0; t < TIMESTEPS; t++) {
    // Вектор обновлёного скрытого состояния (hidden).
    float new_h[HIDDEN] = {0};
    // Поток преобразования входных данных (проходим по всем нейронам скрытого состояния и расчитываем его новое значение):
    // Цель: из текущего вектора входных данных и вектора скрытого состояния полученого на предыдущей итерации
    // получить вектор обновлёного скрытого состояния.
    for (int j = 0; j < HIDDEN; j++) {
      // Расчёт нового значения для нейрона скрытого слоя начинается с учёта Байесовского смещения для него.
      float sum = b_h[j];
      // x_t * W_x
      // Умножаем вектор входных данных на соответсвующий текущему нейрону вектор весов.
      for (int k = 0; k < FEATURES; k++)
        sum += input[t][k] * W_x[k][j];
      // h_{t-1} * W_h
      // Умножаем вектор скрытого состояния полученый на предыдущей итерации на соответсвующий текущему нейрону вектор весов.
      for (int k = 0; k < HIDDEN; k++)
        sum += hidden[k] * W_h[k][j];
      // tanh активация (сжимает значения в диапазон [-1, 1]).
      new_h[j] = tanh(sum);
    }
    // Обновляем скрытое состояние: h_t = new_h.
    memcpy(hidden, new_h, sizeof(hidden));
  }

  // Реализация полносвязного слоя для получения предсказаний (Dense).
  // Поток преобразования вектора скрытого состояния в вектор распределения вероятностей для всех категорий.
  float logits[OUTPUT];
  for (int o = 0; o < OUTPUT; o++) {
    // Расчёт вероятности для текущей категории начинается с учёта Байесовского смещения.
    float sum = b_y[o];
    // Умножаем вектор скрытого состояния на соответсвующий текущей категории вектор весов.
    for (int j = 0; j < HIDDEN; j++)
      sum += hidden[j] * W_y[j][o];
    // Получаем логиты для всех категорий.
    logits[o] = sum;
  }

  // Softmax преобразует логиты для всех категорий в распределения вероятностей для всех категорий.
  // Находим максимальный логит.
  float maxLogit = logits[0];
  for (int i = 1; i < OUTPUT; i++)
    if (logits[i] > maxLogit) maxLogit = logits[i];
  // Считаем сумму экспонент для всех логитов.
  float sumExp = 0;
  for (int i = 0; i < OUTPUT; i++) {
    logits[i] = exp(logits[i] - maxLogit);
    sumExp += logits[i];
  }
  // Получаем распределения вероятностей для всех категорий.
  for (int i = 0; i < OUTPUT; i++)
    output[i] = logits[i] / sumExp;
}